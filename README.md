# Neural Networks

My implementation of several types of neural networks:

- [Radial Basis Function Network](https://en.wikipedia.org/wiki/Radial_basis_function_network)
    - real-valued function {\textstyle \varphi }{\textstyle \varphi } whose value depends only on the distance between the input and some fixed point, either the origin, so that {\textstyle \varphi (\mathbf {x} )=\varphi (\left\|\mathbf {x} \right\|)}{\textstyle \varphi (\mathbf {x} )=\varphi (\left\|\mathbf {x} \right\|)}, or some other fixed point {\textstyle \mathbf {c} }{\textstyle \mathbf {c} }, called a center, so that {\textstyle \varphi (\mathbf {x} )=\varphi (\left\|\mathbf {x} -\mathbf {c} \right\|)}{\textstyle \varphi (\mathbf {x} )=\varphi (\left\|\mathbf {x} -\mathbf {c} \right\|)}. Any function {\textstyle \varphi }{\textstyle \varphi } that satisfies the property {\textstyle \varphi (\mathbf {x} )=\varphi (\left\|\mathbf {x} \right\|)}{\textstyle \varphi (\mathbf {x} )=\varphi (\left\|\mathbf {x} \right\|)} is a radial function. The distance is usually Euclidean distance, although other metrics are sometimes used. They are often used as a collection {\displaystyle \{\varphi _{k}\}_{k}}{\displaystyle \{\varphi _{k}\}_{k}}which forms a basis for some function space of interest, hence the name.
- [Hopfield Network](https://en.wikipedia.org/wiki/Hopfield_network)
    - recurrent artificial neural network popularized by John Hopfield in 1982, but described earlier by Little in 1974. Hopfield nets serve as content-addressable ("associative") memory systems with binary threshold nodes. They are guaranteed to converge to a local minimum and, therefore, may converge to a false pattern (wrong local minimum) rather than the stored pattern (expected local minimum)[citation needed]. Hopfield networks also provide a model for understanding human memory.